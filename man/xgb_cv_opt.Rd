% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgb_cv_opt.R
\name{xgb_cv_opt}
\alias{xgb_cv_opt}
\title{Bayesian Optimization for XGboost(Cross Validation)}
\usage{
xgb_cv_opt(data, label, objectfun, evalmetric, n_folds, eta_range = c(0.1,
  1L), max_depth_range = c(4L, 6L), nrounds_range = c(70, 160L),
  subsample_range = c(0.1, 1L), bytree_range = c(0.4, 1L),
  init_points = 4, n_iter = 10, acq = "ei", kappa = 2.576, eps = 0,
  optkernel = list(type = "exponential", power = 2), classes = NULL,
  seed = 0, nthread = 1)
}
\arguments{
\item{data}{data}

\item{label}{label for classification}

\item{objectfun}{Specify the learning task and the corresponding learning objective
\itemize{
    \item \code{reg:linear} linear regression (Default).
    \item \code{reg:logistic} logistic regression.
    \item \code{binary:logistic} logistic regression for binary classification. Output probability.
    \item \code{binary:logitraw} logistic regression for binary classification, output score before logistic transformation.
    \item \code{multi:softmax} set xgboost to do multiclass classification using the softmax objective. Class is represented by a number and should be from 0 to \code{num_class - 1}.
    \item \code{multi:softprob} same as softmax, but prediction outputs a vector of ndata * nclass elements, which can be further reshaped to ndata, nclass matrix. The result contains predicted probabilities of each data point belonging to each class.
    \item \code{rank:pairwise} set xgboost to do ranking task by minimizing the pairwise loss.
  }}

\item{evalmetric}{evaluation metrics for validation data. Users can pass a self-defined function to it. Default: metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking).
\itemize{
  \item \code{error} binary classification error rate
  \item \code{rmse} Rooted mean square error
  \item \code{logloss} negative log-likelihood function
  \item \code{auc} Area under curve
  \item \code{merror} Exact matching error, used to evaluate multi-class classification
}}

\item{n_folds}{K for cross Validation}

\item{eta_range}{The range of eta(default is c(0.1, 1L))}

\item{max_depth_range}{The range of max_depth(default is c(4L, 6L))}

\item{nrounds_range}{The range of nrounds(default is c(70, 160L))}

\item{subsample_range}{The range of subsample rate(default is c(0.1, 1L))}

\item{bytree_range}{The range of colsample_bytree rate(default is c(0.4, 1L)}

\item{init_points}{Number of randomly chosen points to sample the
target function before Bayesian Optimization fitting the Gaussian Process.}

\item{n_iter}{Total number of times the Bayesian Optimization is to repeated.}

\item{acq}{Acquisition function type to be used. Can be "ucb", "ei" or "poi". #' \itemize{
  \item \code{ucb} GP Upper Confidence Bound
  \item \code{ei} Expected Improvement
  \item \code{poi} Probability of Improvement
}}

\item{kappa}{kappa tunable parameter kappa of GP Upper Confidence Bound, to balance exploitation against exploration,
increasing kappa will make the optimized hyperparameters pursuing exploration.}

\item{eps}{tunable parameter epsilon of Expected Improvement and Probability of Improvement, to balance exploitation against exploration,
increasing epsilon will make the optimized hyperparameters are more spread out across the whole range.}

\item{optkernel}{Kernel (aka correlation function) for the underlying Gaussian Process. This parameter should be a list
that specifies the type of correlation function along with the smoothness parameter. Popular choices are square exponential (default) or matern 5/2}

\item{classes}{set the number of classes. To use only with multiclass objectives.}

\item{seed}{set seed.(default is 0)}

\item{nthread}{set the number number of threads xgboost should use. Default: 1}
}
\value{
The score you specified in the evalmetric option and a list of Bayesian Optimization result is returned:
\itemize{
  \item \code{Best_Par} a named vector of the best hyperparameter set found
  \item \code{Best_Value} the value of metrics achieved by the best hyperparameter set
  \item \code{History} a \code{data.table} of the bayesian optimization history
  \item \code{Pred} a \code{data.table} with validation/cross-validation prediction for each round of bayesian optimization history
}
}
\description{
Bayesian Optimization for XGboost (Cross Validation)
}
\examples{
library(MlBayesOpt)

set.seed(71)
res0 <- xgb_cv_opt(data = iris,
                   label = Species,
                   objectfun = "multi:softmax",
                   evalmetric = "mlogloss",
                   n_folds = 3,
                   classes = 3,
                   init_points = 2,
                   n_iter = 1)

}
